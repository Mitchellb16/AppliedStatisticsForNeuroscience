{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B - Review of Descriptive Statistics with Pandas and Seaborn\n",
    "\n",
    "We'll begin by going over both key notions from probability and some of the most fundamental statistics -- means, medians, and so on. We'll also take this opportunity to practice using the technological tools we'll need for this course. \n",
    "\n",
    "If you've never done any computer programming before, make sure to work through the [Codecademy Python course](https://www.codecademy.com/learn/python) through Lesson 9 as soon as possible. You won't necessarily need it to make it through this lab, but baseline comfort with programming is a key skill for this course and for most of science today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What is a statistic?\n",
    "\n",
    "A *statistic* is a number or collection of numbers that we calculate from a dataset. Examples of statistics include the average, the largest or smallest value, the largest and smallest values, the first or third or $N$th value, or even the entire dataset.\n",
    "\n",
    "One common goal we have when calculating statistics is to *describe* the data. This is called, sensibly enough, *descriptive statistics*. In the field of statistics, we study the properties of these descriptions: which datasets do they summarize well? when does a difference in a statistic mean the datasets are different? how can I tell whether one statistic or another is a better summary of the data?\n",
    "\n",
    "Our goal is *understanding* data -- being able to build models that describe it well or being able to predict the behavior of a system well enough to control it (e.g. to fix it when it is broken, as in medicine).\n",
    "\n",
    "To get our hands on some statistics, we first need a dataset. We'll be working with two. For concreteness' sake, let's say one represents the number of spikes produced by a neuron in response to repeated presentations of some stimulus and the other represents the reaction times of a subject during a cognitive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes our plots show up inside Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy - linear algebra and matrices for python\n",
    "import numpy as np\n",
    "\n",
    "# pandas - \"DataFrames\" to organize our data\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib - workhorse plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn - easy plotting for statistical visualizations\n",
    "#   based off of matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "import util.utils as utils\n",
    "import util.shared as shared\n",
    "\n",
    "shared.format_plots() #makes plots easier to read\n",
    "# (uncomment the line below to make them colorblind-friendly!\n",
    "#shared.format_plots(colorblind=True)\n",
    "shared.format_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin, once we've loaded our data, by printing it out in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spike = pd.read_csv('./data/spikecounts.csv', index_col=0)\n",
    "df_times = pd.read_csv('./data/reactiontimes.csv', index_col=0)\n",
    "\n",
    "df_spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as descriptions of a dataset go, a list of every data point is an accurate one, but not a very useful one.\n",
    "\n",
    "Next, we'll go over how to plot our whole dataset at once, getting a \"visual summary\" that can give us some intuition for what's going on in our data. Then, we'll go over some descriptive statistics that can numerically summarize our data and methods for visualizing those statistics.\n",
    "\n",
    "But first, we'll talk a bit about how we organize our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing Data\n",
    "\n",
    "In the last lab, we talked about two ways of representing probability mass functions: as lists and as dictionaries. Lists and dictionaries are examples of what are called *data structures*. Data structures help us organize information when working with computer programs. Lists and dictionaries are extremely flexible data structures that can be used for a wide variety of purposes -- even implementing whole programming languages! \n",
    "\n",
    "We're interested in a somewhat smaller range of applications: statistical operations on complex datasets. For this, we turn to a specialized kind of \"rectangular dictionary\": the *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Dataframes\n",
    "\n",
    "A list would be sufficent for what we'll be doing in this lab, where all we have is a list of data values.\n",
    "However, in neuroscience we often have to deal with complicated datasets -- a full \"data point\" from a neuroscience experiment might include several numbers (input stimulus and neural response) along with a host of metadata -- subject ID, brain region, genotype, experiment date, and so on. In principle, it's possible to organize all of this information in a collection of arrays, lists, dictionaries, and so on, but it's easy to make mistakes and hard to keep everything in agreement.\n",
    "\n",
    "The standard solution to this problem is a data structure called a *dataframe* or a *table*. A dataframe is like a two-dimensional array that isn't restricted to holding only numbers and that uses either numbers or strings as indices. That might remind you of a dictionary, and it should: a dataframe is like a dictionary of arrays that are all the same length.\n",
    "\n",
    "In Python, the most popular implementation of dataframes is in the `pandas` library. To start getting used to pandas, we'll use it in this lab, even though it isn't strictly necessary. If you'd like to learn more about how to use pandas, check out the tutorial on using pandas in the `Tech Tools Tutorials/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how our data is organized, let's start visualizing it. Pandas dataframes work well with a statistical visualization package called `seaborn`, on which there's more in the tutorial *Plotting with Seaborn*. That tutorial is meant to serve as a reference for most of the plotting we'll need to do through the course, so there's no need to work through the entirety of it just now. We'll also discuss plotting with seaborn and matplotlib in more detail below.\n",
    "\n",
    "You might be familiar with *scatterplots*, which show pairs of data values as points in a two-dimensional plane. The one-dimensional equivalent of a scatterplot is called a *rugplot*, after its passing resemblance to a shag carpet. In a rugplot, we simply place a small tickmark at each number where we observed a data value, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns is our alias for seaborn\n",
    "#  and we call the rugplot function \n",
    "\n",
    "sns.rugplot(df_times);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a sense of where the data values fall -- they appear to be clustered somewhat around 5.5 or 6, and to be somewhat evenly distributed on both sides of that value.\n",
    "\n",
    "Now, use `sns.rugplot` on the spike data, `df_spike`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(df_spike);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rugplot is much less informative.\n",
    "\n",
    "#### Q1 Can you explain why? Think back to the distinction between probability mass functions and probability density functions in the first half of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly-used plot for visualizing data distributions, the *histogram*, is able to handle both kinds of datasets. To make a histogram, we split the data axis into a number of discrete \"bins\" and count how many data values fall into each bin. Often, we divide the number in each bin by the total number of data points in order to get a ratio.\n",
    "\n",
    "**Challenge question**: such a histogram is a probability distribution. What does it describe the probability of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In seaborn, the function we use to plot histograms is called `distplot`. It's capable of producing histograms, rugplots, and another visualization called a *kernel density estimate* (KDE) in any combination. For more information on how to use this function, including how to improve the style, check out the section on *Visualizing Distributions* in the *Plotting with Seaborn* tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_times, #which data to use?\n",
    "             hist=True,kde=True, #plot histogram/kde/both?\n",
    "             rug=True, #include a rugplot?,\n",
    "             bins=5\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply it to the spikes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_spike, hist=True, kde=True, bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is certainly a much better picture of the data than the rugplot gave, but the bins don't line up nicely with our data. This is because seaborn has to guess where to put the bins, and finding the right bin size and location is as much art as science.\n",
    "\n",
    "Adding the `bins` argument to distplot lets you set the location of the bins instead. You can either give a number, and distplot will place that many evenly-sized bins where it thinks is best, or you can give a list of locations for the left edges of the bins. First, play around with different numbers of bins for the reaction times dataset. Then, try fixing the histogram for the spikes data so that it represents the data better. *Hint*: calling the function `range` (i.e., typing in `range(N)`) will give you a list of numbers between `0` and `N-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, thanks to ubiquitous, inexpensive, and easy-to-use computation and communication, building high-quality visualizations of your data is a breeze, you can iterate over multiple different visualizations (e.g. histograms with different bin sizes) in seconds, and you can share those visualizations instantly with anyone with an internet connection.\n",
    "\n",
    "Before these happy days, there was more interest in simple descriptions of datasets that could be easily computed, compared against tables, and shared with colleagues -- where is the \"center\" of the data? is the data more likely to be above the center or below the center? are there any data points that fall very far away from the others?\n",
    "\n",
    "These simple descriptions, which are all calculated from the dataset and so are all *statistics*, are still incredibly useful for distilling information out of data. One of the major goals of this course is to learn what these statistics do and do not tell you, what assumptions about your data are necessary to use them, and when they can lead you astray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of \"Center\": Mean and Median\n",
    "\n",
    "### Mean\n",
    "\n",
    "Perhaps the most commonly-used statistic is the mean, also known as the average value or the *expected value*. To calculate the mean of a dataset, we simply add up all of the values and divide by the size of the dataset. We write this mathematically as:\n",
    "\n",
    "$$\\text{mean} = \\frac{1}{N} \\sum_i{x_i}$$\n",
    "\n",
    "Where the capital Greek letter $\\Sigma$ (\"Sigma\", pronounced like \"S\") means \"add up\" or \"**s**um\".\n",
    "\n",
    "Below, implement a function that computes and returns the mean of a dataset using a `for` loop. The cell that begins \"`assert`\" checks to make sure that your function did the right thing by comparing the output of your function to the output of numpy's mean function. If you're getting an `AssertionError` when you run the cell where `find_mean` is called, that means your mean function isn't doing the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template:\n",
    "```python\n",
    "def find_mean(dataset):\n",
    "    N = ?\n",
    "    sum_total = ?\n",
    "    return mean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert find_mean(df_times.times) == np.mean(df_times.times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important not to confuse the math term *expected* with our intuition about what we \"expect\" from a random event. For example, the \"expected outcome\" of rolling a die is 3.5, but you should not \"expect\" to see 3.5 when you roll a die. The [psychological notion of expectation](https://en.wikipedia.org/wiki/Expectation_%28epistemic%29) is closer to the mathematical concept of [mode](https://en.wikipedia.org/wiki/Mode_%28statistics%29).\n",
    "\n",
    "The mean can also be defined as the point that minimizes the squared error, where the squared error when you guess $a$ and the answer is $b$ is $(a-b)^2$. That is, imagine we're playing a game where you pick a point on the number line and I pick a random point from the dataset. We then calculate the squared error, and you earn more money the smaller that value is. The mean is the value to pick that will maximize your winnings.\n",
    "\n",
    "If you're not one for games, here's one more way to think of the mean. Recall that we introduced probability mass and density functions by a physical analogy, where probability was analogous to mass. Continuing that analogy, we can understand the mean as the \"balancing point\" of an object described by a given probability mass or density function -- as indicated in the image below, it's the place where you would put the wedge of a see-saw in order to maintain balance. In physics, this is called the \"center of mass\".\n",
    "\n",
    "![expectation](img/expectation.png) Modified image from [Wikipedia](https://en.wikipedia.org/wiki/File:Beta_first_moment.svg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "The *median* is another statistic used to quantify the \"center\" of a dataset. The median of a dataset is the value \"in the middle\" of all of points in a dataset: as many data values are above the median as are below the median. If there is an odd number of points in a dataset, the median is a data point -- $\\frac{N-1}{2}$ points are above and below this point. If the number of data points is even, then there is not a unique point that has half of the data above and below it. The dataset splits into two halves, each of size $\\frac{N}{2}$, and any point between the maximum of the smaller half and the minimum of the larger half has the same number of points above it as below. We choose the median to be halfway between these two numbers.\n",
    "\n",
    "Defining a function that calculates the median is harder than for the mean. If you need to calculate the median, use the `np.median` function on the dataset or call the `.median()` method of a pandas column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.median(df_times.times) == df_times.times.median() #these are the same thing!\n",
    "assert df_times.times.median() == df_times['times'].median() #two equivalent ways to write the pandas version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median can also be viewed as the winning strategy in a game, just like the mean. Imagine we play the same game, where you pick a point on the number line and I pick a data point at random, but now, we measure the difference between the two points instead of the squared error: $\\lvert a - b \\rvert$ instead of $(a-b)^2$. The new winning strategy is to pick the median, instead of the mean.\n",
    "\n",
    "The \"center of mass\" interpretation of the mean made it easy to generalize the mean to data with more than one dimension. It's natural to ask if we can generalize the median in the same way. Unfortunately, the point with as much data on one side as the other and the point that wins the game described above are not the same when the data has two or more dimensions. As such, there's not an obvious way to extend the median to multi-dimensional data. See the discussion [here](https://en.wikipedia.org/wiki/Median#Multivariate_median) for a few alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Means versus Medians\n",
    "\n",
    "Both the mean and the median claim to measure the \"center\" of a dataset. So which is better? The answer depends on your data. Specifically, it depends on whether your data is *skewed* or not -- whether values far away from the mean have a tendency to be either big or small, rather than being evenly distributed. For example, incomes are highly skewed upwards, since there are a small number of individuals with incomes orders of magnitude greater than the majority of individuals. On the other hand, human heights are roughly evenly distributed around a central value of 5 feet, 6 inches, and it is roughly as uncommon to find someone half again as tall (around 8 feet) as it is to find someone two-thirds that height (around 3 feet, 6 inches)\n",
    "\n",
    "When we encounter a new dataset, how are we to know whether it is skewed or not? The remainder of our descriptive statistics, and especially our visualizations thereof, will help us answer that question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of \"Spread\": Variance and Percentiles\n",
    "\n",
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our discussion of the mean, we imagined playing a game where I picked random datapoints from the dataset and you won more money the smaller the squared value of the error was. The mean was the number to pick that would win you the most money.\n",
    "\n",
    "If you wanted to know how much money you'd expect to win or lose, you'd have to calculate the mean squared error. In mathematical terms, that would be:\n",
    "\n",
    "$$\\text{mean squared error} = \\frac{1}{N} \\sum_i (x_i-\\mu)^2 $$\n",
    "\n",
    "where the Greek letter $\\mu$ stands for the mean. This quantity is called the *variance* of your data. The bigger it is, the further your datapoints are from the mean, on average.\n",
    "\n",
    "Below, define a function that will compute the variance of a dataset. I'd suggest using the `find_mean` function you've already written! Again, an `AssertionError` means your `find_variance` function isn't doing the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def find_variance(dataset):\n",
    "    mean = find_mean(dataset)\n",
    "    variance = ?\n",
    "    return variance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(np.var(df_times.times), find_variance(df_times.times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance has a physical counterpart, just like the mean. While the mean tells you where a distribution will \"balance\", the variance tells you how hard it would be to \"spin\" the data around that point. The technical term for this in physics is the *moment of inertia*.\n",
    "\n",
    "What are the units of the variance? If our measurements were in rate or time, then our variance has units of rate$^2$ or time$^2$. This makes it difficult to compare the mean with the variance directly. Instead, we can take the square root of the variance to get a quantity called the *standard deviation*, abbreviated *sd*, *std*, or $\\sigma$ (the lower case Greek \"s\", for **s**tandard deviation, pronounced \"sigma\", and from which the variance gets its nickname $\\sigma^2$).\n",
    "\n",
    "Below, define a function that uses your `find_variance` function to compute the standard deviation. HINT: numpy has a square root function: `np.sqrt`. It works on numbers or on lists of numbers. In the latter case, it gives you a list of square roots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Template:\n",
    "```python\n",
    "def find_std(dataset):\n",
    "    variance = find_variance(dataset)\n",
    "    std = ?\n",
    "    return std\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(np.std(df_times.times), find_std(df_times.times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quartiles and Percentiles\n",
    "\n",
    "We can extend the idea of the median as \"the point with half of the data points below it\" to points with different fractions of the dataset below them. For example, we can ask which point has one quarter of the data below it, or one eighth, or one one-hundredth. For each fraction, we get a collection of points: a point with one quarter, two quarters, and three quarters of the data below it, for example, or all of the points with one, two, three, four, ... up to ninety-nine one-hundredths of the data below them. These give us a sense for how our data is spread out, in addition to where its center is.\n",
    "\n",
    "These points partition, or divide up, our data into equally-sized groups: the data points below the first in the collection, between the first and the second, and so on. In the case of quarters and hundredths, these groups have names: *quartiles* and *percentiles*, respectively. They are named in order: the *first quartile* is all of the data points below the first point in the quartering collection, the *37th percentile* is the collection of data points between the 36th and 37th in the collection that divides our data into one hundred equal groups, and so on.\n",
    "\n",
    "Quartiles are useful for getting a quick sense of how our data is spread out. If the two middle quartiles are the same width, then the data is roughly symmetric -- we have go as far below our middle value as above it in order to cover one quarter of the data points. If one is larger than the other, then our data is more spread out in that direction, since we have to move further in order to cover the same number of data points.\n",
    "\n",
    "These ideas are easier to grasp visually, so below, we'll visualize some examples of both cases with boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, a quick side note: you can calculate the mean, median, standard deviation, and quartiles for every column in a pandas data frame by using the method `.describe()`. It's a handy way to start looking at your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Boxplots with Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the *Points of Significance* article [\"Visualizing Samples with Boxplots\"](https://www.nature.com/nmeth/journal/v11/n2/full/nmeth.2813.html) to learn more about *boxplots*, which are used to visualize a dataset in terms of its descriptive statistics. If you're unable to access that article, check out [this Khan Academy video](https://www.khanacademy.org/math/probability/data-distributions-a1/box--whisker-plots-a1/v/constructing-a-box-and-whisker-plot).\n",
    "\n",
    "Below, we'll focus on the example of a boxplot to learn more about how to use `seaborn` and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn and matplotlib\n",
    "\n",
    "Matplotlib is the premier plotting package for Python. It's based on plotting in MATLAB, which is one of that language's best features. Matplotlib is flexible and powerful enough that other plotting libraries have been built on top of it!\n",
    "\n",
    "With that flexibility comes complexity, and matplotlib can be intimidating to new users. In this course, we'll primarily be doing statistical visualization, so we don't need all of matplotlib's power. We'll make things easier on ourselves by using a plotting library built on top of matplotlib called seaborn that specializes in statistical visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Boxplots\n",
    "\n",
    "Making a standard visualization with no customization is easy in seaborn. We were able to make passable histograms above just by using a single function. We can do the same with a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_times, x='times',);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the default choices that seaborn makes aren't always perfect. For example, this boxplot is too tall and not long enough.\n",
    "\n",
    "Each visualization is a `figure`, and `figure`s have `axes`. Seaborn makes a standard figure and axis and then draws its plots onto the axes. If we want to make changes to the figure or the axes, we will often need to use pyplot functions. To learn more about using pyplot, check out the [online tutorial](http://matplotlib.org/users/pyplot_tutorial.html). For the purposes of this class, we'll introduce new functions as needed.\n",
    "\n",
    "To change the shape of the figure that seaborn is drawing in, we need to make the figure ourselves. We make figures with the function `plt.figure`, and we can change its size by including the keyword input `figsize`. Change the `size` variable in the cell below to a few different values and see what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [12,2]\n",
    "\n",
    "fig = plt.figure(figsize=size)\n",
    "\n",
    "sns.boxplot(data=df_times, x='times');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, seaborn does give us control over the style of the plot that it draws. I've made a few style changes that I think make the plot below look better than the one above. Make your own changes to at least the following plot style elements: `saturation`, `color`, `linewidth`,`orient`. All of these are keyword arguments. You can learn more about them in the\n",
    "[seaborn online documentation](http://seaborn.pydata.org/generated/seaborn.boxplot.html).\n",
    "\n",
    "Note that colors can be English names, [RGB values](), or [hex values](http://www.color-hex.com/color-names.html). Check out the [matplotlib online documentation](http://matplotlib.org/api/colors_api.html) for more information.\n",
    "\n",
    "If you're ambitious and have some experience with coding in Python, try figuring out how to use additional keyword arguments to adjust the style of the boxplot even further. You'll need to check out both the [seaborn docs](http://seaborn.pydata.org/generated/seaborn.boxplot.html) (you're looking for `**kwargs`) and the [pyplot docs](http://matplotlib.org/api/pyplot_api.html) (`ctrl-f .boxplot(` )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk a bit about what this boxplot is telling us. The box in the middle covers the second and third quartiles. Inside the box, a solid line indicates the median value.\n",
    "\n",
    "For this data, the line falls approximately in the middle of the box.\n",
    "\n",
    "#### Q2 What does this suggest about how our data is distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are also sometimes called \"box-and-whisker plots\". The *whiskers* are the two lines extending out from the edges of the box. In the past, these were often used to indicate the maximum and minimum values of the data. However, the maximum and minimum are sensitive to outliers. It is more common now to calculate the *inter-quartile range*, the difference between third quartile and first quartile, and to have the whiskers extend to the furthest points that are still within some multiple of that difference from the edges of the box (usually 1.5). Any data points outside of the reach of the whiskers are plotted on their own. This dataset has no outliers.\n",
    "\n",
    "#### Q3 Why are the maximum and minimum sensitive to outliers, while the IQR is not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to see all of the datapoints at once, as in a rug- or scatterplot, in addition to the summary statistics provided by a boxplot. Luckily, seaborn lets us plot more than one thing on the same axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [12,2]\n",
    "fig = plt.figure(figsize=size)\n",
    "\n",
    "sns.boxplot(data=df_times, x='times',\n",
    "            linewidth=4, color='grey');\n",
    "\n",
    "sns.rugplot(df_times);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, it was easy to combine a rugplot and a boxplot. When we are plotting more than one box, that approach won't work as well. Check out the [`stripplot` function](http://seaborn.pydata.org/generated/seaborn.stripplot.html) and, in the code cell below, use it to plot all of the datapoints on top of the boxplot.\n",
    "\n",
    "Once you've got the basic version working, make the plot easier to read by using at least one of the following arguments: `jitter`, `color`, `alpha`, `size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template:\n",
    "```python\n",
    "size = [12,2]\n",
    "fig = plt.figure(figsize=size)\n",
    "\n",
    "sns.boxplot(data=df_times, x='times',\n",
    "            linewidth=4, color='grey');\n",
    "\n",
    "sns.stripplot(?);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, make a boxplot with datapoints, as above, for the spike counts dataset. Neither `rugplot` nor the default choices for `stripplot` will give you a good visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template:\n",
    "```python\n",
    "size = [12,2]\n",
    "fig = plt.figure(figsize=size)\n",
    "\n",
    "sns.boxplot(data=df_spike, x='count',\n",
    "            linewidth=4, color='grey');\n",
    "\n",
    "sns.stripplot(?);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 Just based off of the boxplot, what can we tell about how this data is distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve this visualization by either adjusting the arguments to `stripplot` (try `alpha` or `jitter`) or looking up the function [`swarmplot`](http://seaborn.pydata.org/generated/seaborn.swarmplot.html), which offers a more modern take on the one-dimensional scatterplot."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
