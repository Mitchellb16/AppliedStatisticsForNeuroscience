{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab A - Visualizing Bootstrapping and One-Sample Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "import util.utils as utils\n",
    "import util.shared as shared\n",
    "\n",
    "shared.format_plots()\n",
    "shared.format_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll visualize some of the key components of bootstrapping: the cumulative distribution functions of the population, the sample, and the bootstrap samples. We'll motivate this new visualization for bootstrapping by considering some of the pitfalls of the visualization in the tutorial.\n",
    "Then, we'll work through bootstrapping for some one-sample hypothesis tests.\n",
    "\n",
    "In the second half of the lab, we'll use bootstrapping to compute confidence intervals and $p$-values for the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technical note: this notebook uses the interactive features of JuPyter notebooks. This means that when you generate a plot, you can keep adding things to that plot, so long as you don't either 1) make a new figure or 2) click the blue \"power button\" in the top-right corner of the figure. You should click this button after each animation terminates.\n",
    "\n",
    "To prevent yourself from accidentally plotting into a figure you didn't mean to plot into, make sure to include a `plt.figure()` in each cell where you're making a plot. You'll want to do this before you do the plotting commands (`sns.distplot`, `plt.plot`,etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Empirical Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping relies on sampling from a particular distribution:\n",
    "the *empirical distribution*,\n",
    "which places probability $\\frac{1}{N}$ on each value in the dataset.\n",
    "We sample from this distribution by randomly drawing,\n",
    "with replacement,\n",
    "data values with equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of an empirical distribution arises quite naturally. However, they're a very tricky thing to visualize correctly. The below set of exercises will show you why we can't write down a probability density function for the empirical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Distributions Don't Have Probability Density Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate a data sample from the standard `uniform` distribution. In the interest of computation time, pick a small sample size, like 5 or 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.uniform(size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the data using `distplot`. Include both the `kde` and the `hist`ogram but not the `rug`plot. Start with a small bin size, like 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template:\n",
    "```python\n",
    "binsize = ?\n",
    "bins = np.arange(start=0, step=binsize, stop=1)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(?, bins=?);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values on the y-axis are sometimes higher than 1. As discussed at the beginning of the course, this is somewhat counter-intuitive.\n",
    "\n",
    "#### Q1 What does the y-axis represent? Why is it OK that it goes above 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, reduce the binsize above to a smaller number -- divide by five or more -- and plot the data again.\n",
    "\n",
    "#### Q2 What happens to the histogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep making the bin size smaller and smaller -- though it starts taking longer and longer for Python to compute the histogram.\n",
    "\n",
    "The \"empirical distribution\" corresponds to a binsize of *exactly* 0.\n",
    "\n",
    "#### Q3 Give an intuitive explanation for why this is the case and why this is a problem for viewing the empirical distribution as a density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we don't really have a probability density for our empirical distribution, so we can't visualize it the way we do other probability densities.\n",
    "\n",
    "When we do want to visualize a data distribution directly, we often \"cheat\" and use something like a rugplot or a scatterplot. \n",
    "\n",
    "#### Q4 Think about the benefits and drawbacks of using the rugplot as a stand-in for the empirical distribution. You might compare it to a kernel density estimate or a stripplot. Write your thoughts down here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Distributions Do Have Cumulative Distribution Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still accurately visualize empirical distributions, we just need a different tool - the cumulative distribution function.\n",
    "\n",
    "Recall that the cumulative distribution function (\"CDF\") of a random variable $X$ tells us, for any value $x$, the chance that the random value $X$ is less than or equal to $x$. Look back to the probability and statistics tutorials at the beginning of the course for a reminder of what the CDF is. The empirical CDF is the cumulative density function of the empirical distribution. The empirical CDF will be calculated by `utils.make_eCDF` and returned as a function.\n",
    "\n",
    "In the cell below, we generate a sample from a normal distribution, calculate its empirical distribution, and then plot the cumulative distribution function for the empirical distribution using `utils.make_eCDF`. Feel free to play around with other random variables (`np.random` has a bunch of samplers, including `.uniform`, `.poisson`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.standard_normal(size=10)\n",
    "\n",
    "eCDF = utils.make_eCDF(data)\n",
    "\n",
    "input_range = np.arange(-3, 3, 0.001)\n",
    "\n",
    "plt.figure()\n",
    "plt.step(input_range, [eCDF(input) for input in input_range])\n",
    "plt.ylabel(r\"$P(X â‰¥ x)$\"); plt.xlabel(r\"$x$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 It should look like a \"staircase\" that goes up in level by $\\frac{1}{N}$ every time it passes a datapoint. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a `rugplot` to your visualization of the CDF using `sns.distplot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 How are these two plots connected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sample eCDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will generate an animation that shows the true cumulative distribution function of a random variable in black along with the empirical cumulative distribution functions (eCDFs, for short) of different samples of the same size. You can control the number of samples with the keyword argument `num_samples` and the sample size with the keyword argument `sample_size`. It defaults to sampling a standard normal variable -- read the documentation (with `util.animate_samples?`) if you'd like to sample a different variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.animate_samples(sample_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the sample size to a small value -- less than 5.\n",
    "\n",
    "#### Q7 How close are the individual eCDFs to the true, population CDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8 What would you say the \"average\" of the eCDFs looks like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the sample_size by a factor of 10 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9 What happens to each eCDF individually? What happens to the eCDFs as a group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will animate the process of taking successively bigger samples. With each frame of the animation, the eCDF of a sample of increasing size will be plotted over the true, population CDF. In each frame, the sample size appears in the top center of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = np.random.standard_normal\n",
    "true_CDF = scipy.stats.norm.cdf\n",
    "\n",
    "input_range = np.arange(-5, 5, 0.01)\n",
    "\n",
    "sample_sizes = [1,2,3,4,5,6,\n",
    "               10,20,30,40,50,60,70]\n",
    "\n",
    "utils.animate_sampling(sampler=sampler,\n",
    "                  true_CDF=true_CDF,\n",
    "                  input_range=input_range,\n",
    "                 sample_sizes=sample_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10 What happens to the eCDF as the number of samples goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change which sample sizes are plotted by providing a list of sample sizes as the keyword argument `sample_sizes`.\n",
    "\n",
    "Set the sample sizes so that they only change by one with each frame of the animation -- for example, the list `[1,2,3,4,5]` works.\n",
    "\n",
    "Once you've got the above working, pick a particular value of x. Watch how the eCDF changes at that point as the sample size goes up.\n",
    "\n",
    "#### Q11 Frame by frame, does the eCDF always get closer to the true CDF at that point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge Q Taking the last question into account, can you describe the sense in which the eCDF is \"getting closer\" to the true CDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bootstrap CDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the bootstrap procedure itself.\n",
    "\n",
    "The cell below will first plot the true CDF of a population.\n",
    "\n",
    "Then, it will draw a sample from that distribution and plot the empirical CDF.\n",
    "\n",
    "Then, it will perform a number of bootstrap resamplings from the empirical distribution. As is usual for the bootstrap, we'll draw, with replacement, as many times as there are points in our dataset. The eCDF for each bootstrap sample is then plotted as a thin, transparent line (with a bright green color).\n",
    "\n",
    "Lastly, it will sample again from the true distribution and plot the CDF for each. These samples have the same size as the original data sample and the bootstrap samples. The bootstrap relies on these (hot pink) CDFs being distributed in roughly the same way as the (bright green) bootstrap CDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = np.random.standard_normal\n",
    "true_CDF = scipy.stats.norm.cdf\n",
    "\n",
    "input_range = np.arange(-5,5,0.01)\n",
    "\n",
    "utils.animate_bootstraps(sampler=sampler,\n",
    "               true_CDF=true_CDF,\n",
    "               input_range=input_range,\n",
    "               sample_size=5,\n",
    "                num_bootstraps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12 How similar do the other sample CDFs look to the bootstrap CDFs? What does this mean for the bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease the `sample_size` to `5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13 What changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like, you can view the CDFs, eCDFs, and bootstrapping behavior of more distributions by changing the `sampler` and `true_CDF` keyword arguments to all of the functions above. Note that `sampler`s come from numpy while `true_CDF`s come from `scipy.stats`. Note that you don't want to *call* the functions (that is, follow them with parentheses), you want to pass the functions themselves as arguments.\n",
    "\n",
    "When you do this, you'll want to change the `input_range` keyword argument -- it's best if it covers 95-99% of the possible values.\n",
    "\n",
    "Some example pairs:\n",
    "\n",
    "1. `np.random.standard_exponential` and `scipy.stats.expon.cdf`\n",
    "1. `np.random.standard_cauchy` and `scipy.stats.cauchy.cdf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll apply bootstrapping to a simple, 1-sample hypothesis test.\n",
    "For this purpose, we'll use some data\n",
    "recorded by\n",
    "[Tim Blanche](https://www-bsac.eecs.berkeley.edu/mysite/profile/?person_id=1360187662)\n",
    "in the\n",
    "[laboratory of Nicholas Swindale](http://www.swindale.ecc.ubc.ca/)\n",
    "at the University of British Columbia.\n",
    "This data was downloaded from the NSF-funded\n",
    "[CRCNS Data Sharing website](crcns.org),\n",
    "which hosts a variety of neuroscience datasets\n",
    "for free use in research in education.\n",
    "\n",
    "The complete data set comprises\n",
    "experiments recording eletrical actvity\n",
    "[extracellularly](http://charlesfrye.github.io/FoundationalNeuroscience/80/)\n",
    "from neurons in\n",
    "in the primary visual cortex\n",
    "of anesthetized cats\n",
    "and awake monkeys\n",
    "while the animals were presented with\n",
    "a variety of visual stimuli.\n",
    "\n",
    "We will focus on a single neuron\n",
    "from the anesthetized cat data.\n",
    "The cat was shown oriented drifting bar stimuli\n",
    "at 18 different angles across 8 trials.\n",
    "The figure below shows the average number of spikes\n",
    "that this cell produced in response to stimuli at each orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tuning curve](./img/cat_cell_polar_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots like these are known as *tuning curves*,\n",
    "since they show to which stimulus the neuron is *tuned*,\n",
    "using a metaphor from the analog circuits\n",
    "that were typical of technology in the mid-to-late 20th century,\n",
    "when these curves were in vogue.\n",
    "\n",
    "Below, we'll use bootstrapping to test the hypothesis\n",
    "that this neuron fires more spikes in response to bars\n",
    "oriented between $110^\\circ$ and $60^\\circ$\n",
    "and to bars orthogonal (aka perpendicular)\n",
    "to those directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load \n",
    "\n",
    "- `spikes_to_preferred_stimulus`, which is an array containing the number of spikes the neuron fired in response to stimuli from the preferred direction on each of 24 trials\n",
    "- `mean_to_orthogonal_stimulus`, which is the mean of the responses to the orthogonal stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data/data.npz')\n",
    "\n",
    "spikes_to_preferred_stimulus = data[\"spikes_to_preferred_stimulus\"]\n",
    "mean_to_orthogonal_stimulus = data[\"mean_to_orthogonal_stimulus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bootstrap test our 1-sided hypothesis\n",
    "that the neuron fires more in response to the preferred stimulus,\n",
    "we first bootstrap-estimate the sampling distribution\n",
    "of the difference between the sample mean\n",
    "and the mean response to the orthogonal stimulus.\n",
    "That is,\n",
    "we draw repeated samples, with replacement,\n",
    "from our dataset, `spikes_to_preferred_stimulus`,\n",
    "and calculate our test statistic,\n",
    "the difference in mean spike count.\n",
    "\n",
    "We then calculate what fraction\n",
    "of our sampled statistics\n",
    "fail to support our hypothesis:\n",
    "the fraction of bootstrapped means\n",
    "that are not greater than the mean to the orthogonal stimulus.\n",
    "\n",
    "Implement this test in the cells below.\n",
    "You'll most likely need the numpy functions\n",
    "`np.random.choice` and `np.mean`.\n",
    "You can implement this with a `for` loop\n",
    "or with a list comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template:\n",
    "```python\n",
    "num_samples = len(spikes_to_preferred_stimulus)\n",
    "num_bootstraps = 10000\n",
    "\n",
    "bootstraps = []\n",
    "for _ in range(num_bootstraps):\n",
    "    bootstraps.append(np.random.choice(?, size=num_samples))\n",
    "\n",
    "bootstrap_delta_means = [np.mean(?) - ? for bootstrap in bootstraps]\n",
    "```\n",
    "\n",
    "```python\n",
    "p = np.mean(np.less_equal(bootstrap_delta_means, ?))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "neur299",
   "language": "python",
   "name": "neur299"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
