{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B - Multiple Comparisons and ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Originally by Brandon Rhodes, for use with Pandas*/\n",
       "body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes our plots show up inside Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import util.utils as utils\n",
    "import util.shared as shared\n",
    "\n",
    "shared.format_plots()\n",
    "shared.format_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When N-way ANOVAs are performed in an exploratory fashion (i.e., without pre-specifying which interactions are of interest), there is a \n",
    "[rarely-acknowledged multiple-comparisons effect](https://arxiv.org/pdf/1412.3416)\n",
    "that\n",
    "[massively increases the error rate](http://deevybee.blogspot.co.uk/2013/06/interpreting-unexpected-significant.html).\n",
    "\n",
    "Below, we'll repeatedly simulate null-distributed data for an N-way ANOVA and then perform statistical testing. Because we know *a priori* that the null hypothesis is true, we can interpret any finding as a false positive and so get a sense of our error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate data by generating a single Gaussian random variable, `outcome`, and then assigning it at random to one of two factor levels in each of a number of factors given by `num_factors`. The factor levels are coded as `0` and `1` and factors are labeled with letters `a-z`. This is done for each of `num_subjects` subjects. The results are returned as a pandas dataframe.\n",
    "\n",
    "In a neuroscience context, a dataset like this might arise when dealing with human subjects. Say we are interested, so our outcome is some $z$-scored measure of performance on a memory task. It's quite easy to find an unending stream of binary factors that might influence memory: does the subject regularly get 8 or more hours of sleep? are they regular coffee drinkers? are they suffering from a neurodegenerative disease? have they achieved a terminal degree? do they exercise regularly? And so on. The temptation, for the cautious scientist, is to include all of these factors, for fear of missing something. As we shall see, this is not a good idea.\n",
    "\n",
    "The ANOVA is run using the linear-model-fitting tools in `statsmodels`. The results are also returned as a pandas dataframe. The `p` value is in the column `PR(>F)`.\n",
    "\n",
    "We can control the power of the study by changing the number of subjects and the standard deviation.\n",
    "\n",
    "Note that if you're looking at ANOVAs with more factors, you might start coming across linear algebra errors due to the sample size being too low. If that happens, increase the number of subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.565795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.071758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.311423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.970418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.774602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.905214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.043955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.467722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.625611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a    b    c    d    e    f    g   outcome\n",
       "990  1.0  0.0  0.0  0.0  1.0  1.0  1.0  1.565795\n",
       "991  0.0  1.0  1.0  0.0  1.0  0.0  0.0 -0.071758\n",
       "992  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.311423\n",
       "993  0.0  0.0  0.0  1.0  1.0  0.0  1.0  1.970418\n",
       "994  0.0  1.0  0.0  1.0  0.0  1.0  0.0 -1.774602\n",
       "995  0.0  1.0  1.0  0.0  1.0  1.0  1.0  0.905214\n",
       "996  1.0  1.0  0.0  1.0  1.0  1.0  1.0  2.043955\n",
       "997  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.467722\n",
       "998  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.235248\n",
       "999  1.0  1.0  0.0  1.0  0.0  0.0  0.0 -1.625611"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subjects = 1000\n",
    "num_factors = 7\n",
    "standard_dev = 1\n",
    "\n",
    "null_data = utils.generate_data(num_subjects,num_factors,standard_dev)\n",
    "\n",
    "null_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = utils.run_ANOVA(null_data)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will run this simulated experiment multiple times, tracking three things for each experiment: the fraction of tests that were false positives, whether any false positive occurred, and the result of an \"omnibus\" test (see question 5 for more information). The results are stored in lists, where the element at index `i` is from the `i`th simulated experiment. Run it with the settings below.\n",
    "\n",
    "Ignore the `omnibus_test` code for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiments(num_experiments, num_subjects, num_factors, standard_dev):\n",
    "    \n",
    "    familywise_errors = np.zeros(num_experiments)\n",
    "    false_positive_rates = np.zeros(num_experiments)\n",
    "    omnibus_results = np.zeros(num_experiments)\n",
    "    \n",
    "    for experiment in range(num_experiments):\n",
    "        result = utils.run_ANOVA(utils.generate_data(num_subjects,\n",
    "                                                     num_factors,\n",
    "                                                     standard_dev))[:-1]\n",
    "        false_positive_rates[experiment] = sum(result[\"PR(>F)\"]<0.05)/len(result[\"PR(>F)\"])\n",
    "        familywise_errors[experiment] = false_positive_rates[experiment]>0\n",
    "        omnibus_results[experiment] = omnibus_test(result)\n",
    "        \n",
    "    return false_positive_rates, familywise_errors, omnibus_results\n",
    "\n",
    "def omnibus_test(result):\n",
    "    \n",
    "    model = result[:-1]\n",
    "    residual = result.iloc[-1]\n",
    "    \n",
    "    dof = np.sum(model['df'])\n",
    "    \n",
    "    meansquare_explained = np.sum(model['sum_sq'])/dof\n",
    "    \n",
    "    meansquare_unexplained = residual['sum_sq']/residual['df']\n",
    "    \n",
    "    F = meansquare_explained/meansquare_unexplained\n",
    "    \n",
    "    return (1-scipy.stats.distributions.f.cdf(F,dof,residual['df']))<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "\n",
    "num_subjects = 2500\n",
    "num_factors = 3\n",
    "standard_dev = 0.5\n",
    "\n",
    "FPRs, FWERs, omnis = run_experiments(num_experiments, num_subjects, num_factors, standard_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 Use your results to calculate both the average family-wise error rate (the chance you get at least one (falsely) signficant result), and the average false positive rate (the fraction of signficant results). Explain your results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0471428571429\n",
      "0.29\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(FPRs))\n",
    "\n",
    "print(np.mean(FWERs))\n",
    "\n",
    "print(np.mean(omnis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='1874CD'> **The familywise error rate is around 30% for three factors. The false positive rate remains at 5%. The false positive rate only controls the chance that an individual test is falsely positive, so it's powerless to control the familywise error rate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Note that you did not calculate a false discovery rate. What is the FDR for these experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='1874CD'> **The false discovery rate is 100%, so long as there is any discovery at all. Because the null hypothesis is true in every case, there are no true positives. If there are no discoveries, the FDR is undefined.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 What do you predict would be the effect of increasing the number of factors (to, e.g., 7) on the FPR and FWER? Check your prediction against the simulation and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='1874CD'> ** It should increase the familywise error rate but leave the false positive rate unchanged. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 Make a prediction for the effect of increasing the power (by adding more subjects or decreasing the standard deviation). Check your prediction against the simulation and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='1874CD'> ** Though one might expect power to help here, it actually has no effect. Higher power allows us to discover smaller and smaller effects but does not guarantee that any effects we find are less likely to be false. If we find a large effect with a high power, we can be more confident that the result is not due to chance, but that doesn't directly affect the FPR or FWER.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"omnibus test\" works as follows: first, check whether the model as a whole has a statistically significant mean-square. This can be done by adding the sum-of-squares for each component of the model (except the residuals) and dividing by the sum of the degrees of freedom for each component of the model (again, except the residuals). Then, if the overall result is significant at a level $\\alpha$, perform follow-up $F$-tests. This is analogous to how we used one-way ANOVAs to perform follow-up $t$-tests for one-way ANOVAs.\n",
    "\n",
    "#### Q5 How often would you expect the omnibus test at level $\\alpha$ to fail if the null hypothesis is true for all interactions? Check your prediction against the results of the `omnibus_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='1874CD'> **When the null hypothesis is true for all interactions, the omnibus test should fail with the rate $\\alpha$. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 What happens to the omnibus test if there is even one strong, real effect in the data? Does it still protect against false positives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='1874CD'> ** If there is at least one strong effect in the data, the omnibus test will turn up positive (with a rate given by the power), and we will lose the protection against false positives. That is, we will be unable to tell which of the positives we get are false and which are true. If the prior probability of the hypothesis is small, then these false positives can outweigh the true positives and result in a high false discovery rate. **\n",
    "    \n",
    "** There are some sophisticated strategies to try to avoid this, but the best way to control false positives is to avoid testing gargantuan models with every possible effect and interaction and to instead perform targeted, hypothesis-driven statistical tests.\n",
    "**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "neur299",
   "language": "python",
   "name": "neur299"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
