{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab B - Multiple Comparisons and ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes our plots show up inside Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import util.utils as utils\n",
    "import util.shared as shared\n",
    "\n",
    "shared.format_plots()\n",
    "shared.format_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When N-way ANOVAs are performed in an exploratory fashion (i.e., without pre-specifying which interactions are of interest), there is a \n",
    "[rarely-acknowledged multiple-comparisons effect](https://arxiv.org/pdf/1412.3416)\n",
    "that\n",
    "[massively increases the error rate](http://deevybee.blogspot.co.uk/2013/06/interpreting-unexpected-significant.html).\n",
    "\n",
    "Below, we'll repeatedly simulate null-distributed data for an N-way ANOVA and then perform statistical testing. Because we know *a priori* that the null hypothesis is true, we can interpret any finding as a false positive and so get a sense of our error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate data by generating a single Gaussian random variable, `outcome`, and then assigning it at random to one of two factor levels in each of a number of factors given by `num_factors`. The factor levels are coded as `0` and `1` and factors are labeled with letters `a-z`. This is done for each of `num_subjects` subjects. The results are returned as a pandas dataframe.\n",
    "\n",
    "In a neuroscience context, a dataset like this might arise when dealing with human subjects. Say we are interested, so our outcome is some $z$-scored measure of performance on a memory task. It's quite easy to find an unending stream of binary factors that might influence memory: does the subject regularly get 8 or more hours of sleep? are they regular coffee drinkers? are they suffering from a neurodegenerative disease? have they achieved a terminal degree? do they exercise regularly? And so on. The temptation, for the cautious scientist, is to include all of these factors, for fear of missing something. As we shall see, this is not a good idea.\n",
    "\n",
    "The ANOVA is run using the linear-model-fitting tools in `statsmodels`. The results are also returned as a pandas dataframe. The `p` value is in the column `PR(>F)`.\n",
    "\n",
    "We can control the power of the study by changing the number of subjects and the standard deviation.\n",
    "\n",
    "Note that if you're looking at ANOVAs with more factors, you might start coming across linear algebra errors due to the sample size being too low. If that happens, increase the number of subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = 1000\n",
    "num_factors = 7\n",
    "standard_dev = 1\n",
    "\n",
    "null_data = utils.generate_data(num_subjects,num_factors,standard_dev)\n",
    "\n",
    "null_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = utils.run_ANOVA(null_data)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will run this simulated experiment multiple times, tracking three things for each experiment: the fraction of tests that were false positives, whether any false positive occurred, and the result of an \"omnibus\" test (see question 5 for more information). The results are stored in lists, where the element at index `i` is from the `i`th simulated experiment. Run it with the settings below.\n",
    "\n",
    "Ignore the `omnibus_test` code for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(num_experiments, num_subjects, num_factors, standard_dev):\n",
    "    \n",
    "    familywise_errors = np.zeros(num_experiments)\n",
    "    false_positive_rates = np.zeros(num_experiments)\n",
    "    omnibus_results = np.zeros(num_experiments)\n",
    "    \n",
    "    for experiment in range(num_experiments):\n",
    "        result = utils.run_ANOVA(utils.generate_data(num_subjects,\n",
    "                                                     num_factors,\n",
    "                                                     standard_dev))[:-1]\n",
    "        false_positive_rates[experiment] = sum(result[\"PR(>F)\"]<0.05)/len(result[\"PR(>F)\"])\n",
    "        familywise_errors[experiment] = false_positive_rates[experiment]>0\n",
    "        omnibus_results[experiment] = omnibus_test(result)\n",
    "        \n",
    "    return false_positive_rates, familywise_errors, omnibus_results\n",
    "\n",
    "def omnibus_test(result):\n",
    "    \n",
    "    model = result[:-1]\n",
    "    residual = result.iloc[-1]\n",
    "    \n",
    "    dof = np.sum(model['df'])\n",
    "    \n",
    "    meansquare_explained = np.sum(model['sum_sq'])/dof\n",
    "    \n",
    "    meansquare_unexplained = residual['sum_sq']/residual['df']\n",
    "    \n",
    "    F = meansquare_explained/meansquare_unexplained\n",
    "    \n",
    "    return (1-scipy.stats.distributions.f.cdf(F,dof,residual['df']))<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "\n",
    "num_subjects = 2500\n",
    "num_factors = 3\n",
    "standard_dev = 0.5\n",
    "\n",
    "FPRs, FWERs, omnis = run_experiments(num_experiments, num_subjects, num_factors, standard_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 Use your results to calculate both the average family-wise error rate (the chance you get at least one (falsely) signficant result), and the average false positive rate (the fraction of signficant results). Explain your results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(FPRs))\n",
    "\n",
    "print(np.mean(FWERs))\n",
    "\n",
    "print(np.mean(omnis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Note that you did not calculate a false discovery rate. What is the FDR for these experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 What do you predict would be the effect of increasing the number of factors (to, e.g., 7) on the FPR and FWER? Check your prediction against the simulation and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 Make a prediction for the effect of increasing the power (by adding more subjects or decreasing the standard deviation). Check your prediction against the simulation and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"omnibus test\" works as follows: first, check whether the model as a whole has a statistically significant mean-square. This can be done by adding the sum-of-squares for each component of the model (except the residuals) and dividing by the sum of the degrees of freedom for each component of the model (again, except the residuals). Then, if the overall result is significant at a level $\\alpha$, perform follow-up $F$-tests. This is analogous to how we used one-way ANOVAs to perform follow-up $t$-tests for one-way ANOVAs.\n",
    "\n",
    "#### Q5 How often would you expect the omnibus test at level $\\alpha$ to fail if the null hypothesis is true for all interactions? Check your prediction against the results of the `omnibus_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 What happens to the omnibus test if there is even one strong, real effect in the data? Does it still protect against false positives?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
